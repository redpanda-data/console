input:
  kafka_franz:
    seed_brokers:
      - ${REDPANDA_BROKERS}
    topics:
      - ${TOPIC}
    consumer_group: ${TOPIC}-ai-pipeline
    tls:
      enabled: true
    sasl:
      - username: ${USERNAME}
        password: ${secrets.KAFKA_PASSWORD}
        mechanism: ${SASL_MECHANISM}
pipeline:
  processors:
    - mapping: |
        root.document = content().string()
    - label: embeddings
      branch:
        processors:
          - mutation: |
              if (@kafka_key.not_empty() | null) == null {
                meta kafka_key = content().hash("sha256").encode("hex")
              }
          - text_chunker:
              strategy: recursive_character
          - group_by_value:
              value: ${! @kafka_key }
          - mutation: |
              meta chunk_id = batch_index()
          - mapping: |
              root.document = content().string()
          - openai_embeddings:
              api_key: ${secrets.OPENAI_KEY}
              model: text-embedding-3-small
              text_mapping: this.document
              dimensions: 768
        result_map: root.embeddings = this
output:
  sql_insert:
    driver: postgres
    dsn: ${secrets.POSTGRES_DSN}
    table: ${TOPIC}
    init_statement: |
      CREATE TABLE IF NOT EXISTS ${TOPIC} (
        key text,
        chunk_id integer,
        document text,
        embeddings vector(768),
        PRIMARY KEY(key, chunk_id)
      );
    columns:
      - key
      - chunk_id
      - document
      - embeddings
    args_mapping: root = [ @kafka_key, @chunk_id, this.document, this.embeddings.vector() ]
    suffix: ON CONFLICT (key, chunk_id) DO UPDATE SET document=EXCLUDED.document,
      embeddings=EXCLUDED.embeddings
    max_in_flight: 8
    batching:
      count: 64
      period: 30s
